# -*- coding: utf-8 -*-
"""Solar Enegry.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1P1Xpj0CrEe1O9Gol0jbymcdY13Te25qC
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

df = pd.read_csv('/content/CW_dataset(1).csv') #read the csv file.

df.info

df.isna().sum()

df.dropna(inplace=True)

df.isna().sum()

df.head()

df.describe()

corr_matrix = df.corr() #check for correlation between numerical features.
corr_matrix['use [kW]'].sort_values(ascending = False) #if value is 1 it means there is a strong positive correlation

corr_matrix = df.corr() #check for correlation between numerical features.
corr_matrix['gen [kW]'].sort_values(ascending = False) #if value is 1 it means there is a strong positive correlation

from pandas.plotting import scatter_matrix
attributes = ['gen [kW]','apparentTemperature']
scatter_matrix(df[attributes], figsize = (12, 8))

import seaborn as sns
sns.heatmap(df.corr());

#split the data into train and test set
def split_train_test(data, test_ratio):
  np.random.seed(42)
  shuffled = np.random.permutation(len(data)) #shuffles the indices in the dataset randomly
  print(shuffled)
  test_set_size = int(len(data) * test_ratio)
  test_indices = shuffled[:test_set_size]
  train_indices = shuffled[test_set_size:]
  return data.iloc[train_indices], data.iloc[test_indices]

train_set, test_set = train_test_split(df, test_size = 0.30, random_state = 42)
print(f"Rows in train set: {len(train_set)}\nRows in test set: {len(test_set)}\n")

from sklearn import linear_model
X=train_set[['Furnace 2 [kW]','Furnace 1 [kW]','Living room [kW]','Dishwasher [kW]','Home office [kW]','Fridge [kW]','Wine cellar [kW]','Kitchen 14 [kW]', 'Kitchen 12 [kW]','Kitchen 38 [kW]','Microwave [kW]'	,'Barn [kW]','Well [kW]','Garage door [kW]','pressure','precipIntensity','dewPoint','windBearing']]
y= train_set['use [kW]']
#from sklearn.tree import DecisionTreeRegressor
#model = DecisionTreeRegressor()
regr = linear_model.LinearRegression()
regr.fit(X, y)
print(regr.coef_)

from sklearn.metrics import mean_squared_error
df_predictions = regr.predict(X)
df_predictions
mse= mean_squared_error(y,df_predictions)
rmse=np.sqrt(mse)
rmse

from sklearn.metrics import mean_absolute_error
mae=mean_absolute_error(y,df_predictions)
mae

from sklearn.metrics import r2_score
r2=r2_score(y,df_predictions)
r2

from sklearn.ensemble import RandomForestRegressor

forest = RandomForestRegressor(n_estimators=100, random_state=42)
forest.fit(X,y)
df_pred=forest.predict(X)
forest_mse = mean_squared_error(y,df_pred)
forest_rmse = np.sqrt(forest_mse)
forest_rmse

forest_mae=mean_absolute_error(y,df_pred)
forest_mae

from sklearn.metrics import r2_score
forest_r2=r2_score(y,df_pred)
forest_r2

import pandas as pd
import numpy as np

import matplotlib.pyplot as plt

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

from sklearn.neural_network import MLPRegressor

from sklearn import metrics

from sklearn.model_selection import GridSearchCV

trainX, testX, trainY, testY = train_test_split(X, y, test_size = 0.2)

sc=StandardScaler()

scaler = sc.fit(trainX)
trainX_scaled = scaler.transform(trainX)
testX_scaled = scaler.transform(testX)

mlp_reg = MLPRegressor(hidden_layer_sizes=(150,100,50),
                       max_iter = 300,activation = 'relu',
                       solver = 'adam')

mlp_reg.fit(trainX_scaled, trainY)

y_pred = mlp_reg.predict(testX_scaled)

df_temp = pd.DataFrame({'Actual': testY, 'Predicted': y_pred})

print('Mean Absolute Error:', metrics.mean_absolute_error(testY, y_pred))  
print('Mean Squared Error:', metrics.mean_squared_error(testY, y_pred))  
print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(testY, y_pred)))

print('Root :', np.sqrt(metrics.r2_score(testY, y_pred)))

from sklearn.model_selection import cross_val_score
scores = cross_val_score(regr,X,y, scoring='neg_mean_squared_error', cv=10)
regr_score=np.sqrt(-scores)
regr_score

def display_scores(scores):
    print("Scores:", scores)
    print("Mean:", scores.mean())
    print("Standard deviation:", scores.std())

print('Cross validation scores for Linear regression')
display_scores(regr_score)

scores = cross_val_score(forest,X,y, scoring='neg_mean_squared_error', cv=10)
forest_score=np.sqrt(-scores)
forest_score

print('Cross validation scores for Random forest regression')
display_scores(forest_score)

import numpy as np
from sklearn import datasets
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import learning_curve
import matplotlib.pyplot as plt

train_size,train_score,validation_score = learning_curve(regr,X,y,train_sizes=np.logspace(-1,0,3))

plt.plot(train_size,validation_score.mean(axis=1),lw=2,label='cross-validation')
plt.plot(train_size,train_score.mean(axis=1),lw=2, label='training')
plt.xlabel('number of data set')
plt.ylabel('Accuracy score')
plt.title('Learning Curve:Linear Regression')
plt.legend(['cross validation', 'training score'], loc='best')

train_size,train_score,validation_score = learning_curve(forest,X,y,train_sizes=np.logspace(-1,0,3))

plt.plot(train_size,validation_score.mean(axis=1),lw=2,label='cross-validation')
plt.plot(train_size,train_score.mean(axis=1),lw=2, label='training')
plt.xlabel('number of data set')
plt.ylabel('Accuracy score')
plt.title('Learning Curve:Random forest Regression')
plt.legend(['cross validation', 'training score'], loc='best')

train_size,train_score,validation_score = learning_curve(mlp_reg,trainX_scaled,trainY,train_sizes=np.logspace(-1,0,5))

plt.plot(train_size,validation_score.mean(axis=1),lw=2,label='cross-validation')
plt.plot(train_size,train_score.mean(axis=1),lw=2, label='training')
plt.xlabel('number of data set')
plt.ylabel('Accuracy score')
plt.title('Learning Curve:Neural Network Regression')
plt.legend(['cross validation', 'training score'], loc='best')

from sklearn.svm import SVR
svm_reg = SVR(kernel="linear")
svm_reg.fit(X, y)
df_pred2 = svm_reg.predict(X)
svm_mse = mean_squared_error(y, df_pred2)
svm_rmse = np.sqrt(svm_mse)
svm_rmse

svm_mae=mean_absolute_error(y,df_pred2)
svm_mae

from sklearn.metrics import r2_score
svm_r2=r2_score(y,df_pred2)
svm_r2

train_size,train_score,validation_score = learning_curve(svm_reg,X,y,train_sizes=np.logspace(-1,0,3))

plt.plot(train_size,validation_score.mean(axis=1),lw=2,label='cross-validation')
plt.plot(train_size,train_score.mean(axis=1),lw=2, label='training')
plt.xlabel('number of data set')
plt.ylabel('Accuracy score')
plt.title('Learning Curve: Support Vector Machine')
plt.legend(['cross validation', 'training score'], loc='best')

from sklearn.metrics import mean_squared_error
df_predictions = model1.predict(X) 
tree_mse = mean_squared_error(y, df_predictions)  
tree_rmse = np.sqrt(tree_mse)

tree_rmse

tree_mse

from sklearn.metrics import r2_score
r2=r2_score(y,df_predictions)
r2

from sklearn.model_selection import learning_curve
train_size,train_score,validation_score=learning_curve(model1,X,y,train_sizes=np.logspace(-1,0,4))
plt.plot(train_size,validation_score.mean(axis=1),lw=2,label='training')
plt.plot(train_size,train_score.mean(axis=1),lw=2, label='cross-validation')
plt.xlabel('number of data set')
plt.ylabel('Accuracy score')
plt.title('Learning Curve for Decision Tree')
plt.legend(['cross validation', 'training score'], loc='best')